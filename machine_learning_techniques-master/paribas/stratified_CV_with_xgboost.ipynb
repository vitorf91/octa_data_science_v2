{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#I want to get to know gradient boosting methods (in particular, the xgboost library) and i am also currently in barbados.\n",
    "#Import libraries:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stratified CV w/ XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading & preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data...\n",
      "Clearing...\n"
     ]
    }
   ],
   "source": [
    "#https://www.kaggle.com/director/bnp-paribas-cardif-claims-management/simple-xgboost-0-46146/code\n",
    "print('Load data...')\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "target = train['target']\n",
    "train = train.drop(['ID','target'],axis=1)\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "ids = test['ID'].values\n",
    "test = test.drop(['ID'],axis=1)\n",
    "#\n",
    "print('Clearing...')\n",
    "for (train_name, train_series), (test_name, test_series) in zip(train.iteritems(),test.iteritems()):\n",
    "    if train_series.dtype == 'O':\n",
    "        #for objects: factorize\n",
    "        train[train_name], tmp_indexer = pd.factorize(train[train_name])\n",
    "        test[test_name] = tmp_indexer.get_indexer(test[test_name])\n",
    "        #but now we have -1 values (NaN)\n",
    "    else:\n",
    "        #for int or float: fill NaN\n",
    "        tmp_len = len(train[train_series.isnull()])\n",
    "        if tmp_len>0:\n",
    "            train.loc[train_series.isnull(), train_name] = train_series.mean()\n",
    "        #and Test\n",
    "        tmp_len = len(test[test_series.isnull()])\n",
    "        if tmp_len>0:\n",
    "            test.loc[test_series.isnull(), test_name] = train_series.mean()  #TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/c/bnp-paribas-cardif-claims-management/forums/t/19083/best-practices-for-parameter-tuning-on-models/\n",
    "#https://github.com/dmlc/xgboost/blob/master/demo/guide-python/cross_validation.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit the model...\n",
      "408.922273874\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "xgtrain = xgb.DMatrix(train.values, target.values)\n",
    "xgtest = xgb.DMatrix(test.values)\n",
    "\n",
    "params = {'objective': 'binary:logistic', \n",
    "              'subsample': 1, \n",
    "              'eta': 0.1, \n",
    "              'colsample_bytree': 0.9, \n",
    "              'max_depth': 10,\n",
    "              'min_child_weight' : 5,\n",
    "                 'silent':1}\n",
    "\n",
    "#Now let's fit the model\n",
    "print('Fit the model...')\n",
    "num_round = 50 #1800 CHANGE THIS BEFORE START\n",
    "clf = xgb.cv(params,xgtrain,num_boost_round=num_round,metrics={'logloss'}, nfold = 5 ,\n",
    "             seed = 0 ,maximize=False)\n",
    "\n",
    "#i have attempted this with argument stratified = 1 and get the following error:\n",
    "#TypeError: cv() got an unexpected keyword argument 'stratified'\n",
    "\n",
    "\n",
    "#Make predict\n",
    "# print('Predict...')\n",
    "##check here for eval metrics + https://github.com/dmlc/xgboost/blob/master/demo/guide-python/evals_result.py\n",
    "t1 = time.time()\n",
    "total_time = t1 - t0\n",
    "print total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test-logloss-mean</th>\n",
       "      <th>test-logloss-std</th>\n",
       "      <th>train-logloss-mean</th>\n",
       "      <th>train-logloss-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.657933</td>\n",
       "      <td>0.000591</td>\n",
       "      <td>0.655429</td>\n",
       "      <td>0.000550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.628350</td>\n",
       "      <td>0.000739</td>\n",
       "      <td>0.623086</td>\n",
       "      <td>0.000943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.603884</td>\n",
       "      <td>0.001104</td>\n",
       "      <td>0.595802</td>\n",
       "      <td>0.001063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.583370</td>\n",
       "      <td>0.001012</td>\n",
       "      <td>0.572541</td>\n",
       "      <td>0.000836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.566755</td>\n",
       "      <td>0.000924</td>\n",
       "      <td>0.553194</td>\n",
       "      <td>0.000422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.552614</td>\n",
       "      <td>0.000855</td>\n",
       "      <td>0.536283</td>\n",
       "      <td>0.000151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.540211</td>\n",
       "      <td>0.000855</td>\n",
       "      <td>0.521392</td>\n",
       "      <td>0.000222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.529721</td>\n",
       "      <td>0.000880</td>\n",
       "      <td>0.508403</td>\n",
       "      <td>0.000270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.520790</td>\n",
       "      <td>0.000867</td>\n",
       "      <td>0.497102</td>\n",
       "      <td>0.000514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.513212</td>\n",
       "      <td>0.000905</td>\n",
       "      <td>0.487175</td>\n",
       "      <td>0.000640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.506994</td>\n",
       "      <td>0.000959</td>\n",
       "      <td>0.478506</td>\n",
       "      <td>0.000730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.501447</td>\n",
       "      <td>0.001094</td>\n",
       "      <td>0.470861</td>\n",
       "      <td>0.000729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.496789</td>\n",
       "      <td>0.001237</td>\n",
       "      <td>0.464108</td>\n",
       "      <td>0.000729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.492689</td>\n",
       "      <td>0.001184</td>\n",
       "      <td>0.457947</td>\n",
       "      <td>0.000677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.489190</td>\n",
       "      <td>0.001212</td>\n",
       "      <td>0.452237</td>\n",
       "      <td>0.000786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.486248</td>\n",
       "      <td>0.001257</td>\n",
       "      <td>0.447101</td>\n",
       "      <td>0.000871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.483681</td>\n",
       "      <td>0.001239</td>\n",
       "      <td>0.442427</td>\n",
       "      <td>0.000849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.481431</td>\n",
       "      <td>0.001276</td>\n",
       "      <td>0.438261</td>\n",
       "      <td>0.000909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.479627</td>\n",
       "      <td>0.001380</td>\n",
       "      <td>0.434358</td>\n",
       "      <td>0.000855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.477896</td>\n",
       "      <td>0.001453</td>\n",
       "      <td>0.430595</td>\n",
       "      <td>0.000764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.476538</td>\n",
       "      <td>0.001381</td>\n",
       "      <td>0.427351</td>\n",
       "      <td>0.000817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.475284</td>\n",
       "      <td>0.001424</td>\n",
       "      <td>0.424263</td>\n",
       "      <td>0.000826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.474239</td>\n",
       "      <td>0.001395</td>\n",
       "      <td>0.421472</td>\n",
       "      <td>0.000912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.473294</td>\n",
       "      <td>0.001422</td>\n",
       "      <td>0.418848</td>\n",
       "      <td>0.000894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.472517</td>\n",
       "      <td>0.001478</td>\n",
       "      <td>0.416437</td>\n",
       "      <td>0.000846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.471818</td>\n",
       "      <td>0.001503</td>\n",
       "      <td>0.414173</td>\n",
       "      <td>0.000894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.471244</td>\n",
       "      <td>0.001540</td>\n",
       "      <td>0.412018</td>\n",
       "      <td>0.000970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.470718</td>\n",
       "      <td>0.001583</td>\n",
       "      <td>0.410244</td>\n",
       "      <td>0.001135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.470290</td>\n",
       "      <td>0.001598</td>\n",
       "      <td>0.408401</td>\n",
       "      <td>0.001012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.469917</td>\n",
       "      <td>0.001631</td>\n",
       "      <td>0.406792</td>\n",
       "      <td>0.001054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.469591</td>\n",
       "      <td>0.001632</td>\n",
       "      <td>0.405117</td>\n",
       "      <td>0.001180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.469236</td>\n",
       "      <td>0.001656</td>\n",
       "      <td>0.403349</td>\n",
       "      <td>0.000978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.468996</td>\n",
       "      <td>0.001689</td>\n",
       "      <td>0.401859</td>\n",
       "      <td>0.000776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.468792</td>\n",
       "      <td>0.001664</td>\n",
       "      <td>0.400643</td>\n",
       "      <td>0.000672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.468562</td>\n",
       "      <td>0.001643</td>\n",
       "      <td>0.399290</td>\n",
       "      <td>0.000633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.468300</td>\n",
       "      <td>0.001700</td>\n",
       "      <td>0.397969</td>\n",
       "      <td>0.000679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.468079</td>\n",
       "      <td>0.001699</td>\n",
       "      <td>0.396654</td>\n",
       "      <td>0.000702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.467950</td>\n",
       "      <td>0.001706</td>\n",
       "      <td>0.395548</td>\n",
       "      <td>0.000823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.467791</td>\n",
       "      <td>0.001684</td>\n",
       "      <td>0.394453</td>\n",
       "      <td>0.000800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.467616</td>\n",
       "      <td>0.001657</td>\n",
       "      <td>0.393437</td>\n",
       "      <td>0.000796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.467466</td>\n",
       "      <td>0.001650</td>\n",
       "      <td>0.392171</td>\n",
       "      <td>0.000925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.467381</td>\n",
       "      <td>0.001702</td>\n",
       "      <td>0.391102</td>\n",
       "      <td>0.001146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.467253</td>\n",
       "      <td>0.001774</td>\n",
       "      <td>0.389948</td>\n",
       "      <td>0.000796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.467122</td>\n",
       "      <td>0.001733</td>\n",
       "      <td>0.389037</td>\n",
       "      <td>0.000645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.467043</td>\n",
       "      <td>0.001747</td>\n",
       "      <td>0.387892</td>\n",
       "      <td>0.000712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.466965</td>\n",
       "      <td>0.001757</td>\n",
       "      <td>0.386938</td>\n",
       "      <td>0.000627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.466865</td>\n",
       "      <td>0.001787</td>\n",
       "      <td>0.385881</td>\n",
       "      <td>0.000904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.466820</td>\n",
       "      <td>0.001841</td>\n",
       "      <td>0.384970</td>\n",
       "      <td>0.000891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.466724</td>\n",
       "      <td>0.001895</td>\n",
       "      <td>0.384210</td>\n",
       "      <td>0.000894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.001911</td>\n",
       "      <td>0.383509</td>\n",
       "      <td>0.000787</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    test-logloss-mean  test-logloss-std  train-logloss-mean  train-logloss-std\n",
       "0            0.657933          0.000591            0.655429           0.000550\n",
       "1            0.628350          0.000739            0.623086           0.000943\n",
       "2            0.603884          0.001104            0.595802           0.001063\n",
       "3            0.583370          0.001012            0.572541           0.000836\n",
       "4            0.566755          0.000924            0.553194           0.000422\n",
       "5            0.552614          0.000855            0.536283           0.000151\n",
       "6            0.540211          0.000855            0.521392           0.000222\n",
       "7            0.529721          0.000880            0.508403           0.000270\n",
       "8            0.520790          0.000867            0.497102           0.000514\n",
       "9            0.513212          0.000905            0.487175           0.000640\n",
       "10           0.506994          0.000959            0.478506           0.000730\n",
       "11           0.501447          0.001094            0.470861           0.000729\n",
       "12           0.496789          0.001237            0.464108           0.000729\n",
       "13           0.492689          0.001184            0.457947           0.000677\n",
       "14           0.489190          0.001212            0.452237           0.000786\n",
       "15           0.486248          0.001257            0.447101           0.000871\n",
       "16           0.483681          0.001239            0.442427           0.000849\n",
       "17           0.481431          0.001276            0.438261           0.000909\n",
       "18           0.479627          0.001380            0.434358           0.000855\n",
       "19           0.477896          0.001453            0.430595           0.000764\n",
       "20           0.476538          0.001381            0.427351           0.000817\n",
       "21           0.475284          0.001424            0.424263           0.000826\n",
       "22           0.474239          0.001395            0.421472           0.000912\n",
       "23           0.473294          0.001422            0.418848           0.000894\n",
       "24           0.472517          0.001478            0.416437           0.000846\n",
       "25           0.471818          0.001503            0.414173           0.000894\n",
       "26           0.471244          0.001540            0.412018           0.000970\n",
       "27           0.470718          0.001583            0.410244           0.001135\n",
       "28           0.470290          0.001598            0.408401           0.001012\n",
       "29           0.469917          0.001631            0.406792           0.001054\n",
       "30           0.469591          0.001632            0.405117           0.001180\n",
       "31           0.469236          0.001656            0.403349           0.000978\n",
       "32           0.468996          0.001689            0.401859           0.000776\n",
       "33           0.468792          0.001664            0.400643           0.000672\n",
       "34           0.468562          0.001643            0.399290           0.000633\n",
       "35           0.468300          0.001700            0.397969           0.000679\n",
       "36           0.468079          0.001699            0.396654           0.000702\n",
       "37           0.467950          0.001706            0.395548           0.000823\n",
       "38           0.467791          0.001684            0.394453           0.000800\n",
       "39           0.467616          0.001657            0.393437           0.000796\n",
       "40           0.467466          0.001650            0.392171           0.000925\n",
       "41           0.467381          0.001702            0.391102           0.001146\n",
       "42           0.467253          0.001774            0.389948           0.000796\n",
       "43           0.467122          0.001733            0.389037           0.000645\n",
       "44           0.467043          0.001747            0.387892           0.000712\n",
       "45           0.466965          0.001757            0.386938           0.000627\n",
       "46           0.466865          0.001787            0.385881           0.000904\n",
       "47           0.466820          0.001841            0.384970           0.000891\n",
       "48           0.466724          0.001895            0.384210           0.000894\n",
       "49           0.466667          0.001911            0.383509           0.000787"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some notes on xgb.train() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import KFold, train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(train, target, test_size = 0.05 ,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgtrains = xgb.DMatrix(X_train.values, y_train.values)\n",
    "xgtest = xgb.DMatrix(X_test.values, y_test.values)\n",
    "# xgtest = xgb.DMatrix(test.values)\n",
    "params = {'objective': 'binary:logistic', \n",
    "              'subsample': 1, \n",
    "              'eta': 0.1, \n",
    "              'colsample_bytree': 0.9, \n",
    "              'max_depth': 10,\n",
    "              'min_child_weight' : 5,\n",
    "                 'silent':1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Will train until logloss error hasn't decreased in 10 rounds.\n",
      "[0]\tlogloss-error:0.235963\n",
      "[1]\tlogloss-error:0.232639\n",
      "[2]\tlogloss-error:0.232290\n",
      "[3]\tlogloss-error:0.231065\n",
      "[4]\tlogloss-error:0.229491\n",
      "[5]\tlogloss-error:0.230191\n",
      "[6]\tlogloss-error:0.227567\n",
      "[7]\tlogloss-error:0.226517\n",
      "[8]\tlogloss-error:0.225993\n",
      "[9]\tlogloss-error:0.226867\n",
      "[10]\tlogloss-error:0.226517\n",
      "[11]\tlogloss-error:0.227917\n",
      "[12]\tlogloss-error:0.226168\n",
      "[13]\tlogloss-error:0.227567\n",
      "[14]\tlogloss-error:0.226692\n",
      "[15]\tlogloss-error:0.227042\n",
      "[16]\tlogloss-error:0.227742\n",
      "[17]\tlogloss-error:0.227567\n",
      "[18]\tlogloss-error:0.226692\n",
      "Stopping. Best iteration:\n",
      "[8]\tlogloss-error:0.225993\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clft = xgb.train(params,xgtrains,num_boost_round=num_round,\n",
    "                 evals= [(xgtest,'logloss')] , early_stopping_rounds = 10,\n",
    "                 verbose_eval=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#see here:\n",
    "#https://www.kaggle.com/ashhafez/springleaf-marketing-response/xgb-learning-rate-eta-decay/run/78945/code\n",
    "#http://discuss.analyticsvidhya.com/t/how-to-predict-class-labels-using-xgboost-in-python-when-objective-function-is-binary-logistic/7809"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
