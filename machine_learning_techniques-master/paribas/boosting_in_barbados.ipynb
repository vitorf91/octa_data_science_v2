{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#I want to get to know gradient boosting methods (in particular, the xgboost library) and i am also currently in barbados.\n",
    "#Import libraries:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import time\n",
    "#load data:\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "target = train['target']\n",
    "#drop targets & (unique row) IDs from training data\n",
    "train = train.drop(['ID','target'],axis=1)\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "IDs = test['ID'].values\n",
    "test = test.drop(['ID'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#impute both numerical & categorical features a la\n",
    "#http://stackoverflow.com/questions/25239958/impute-categorical-missing-values-in-scikit-learn\n",
    "\n",
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "class DataFrameImputer(TransformerMixin):\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Impute missing values.\n",
    "\n",
    "        Columns of dtype object are imputed with the most frequent value \n",
    "        in column.\n",
    "\n",
    "        Columns of other types are imputed with mean of column.\n",
    "\n",
    "        \"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "\n",
    "        self.fill = pd.Series([X[c].value_counts().index[0]\n",
    "            if X[c].dtype == np.dtype('O') else X[c].mean() for c in X],\n",
    "            index=X.columns)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return X.fillna(self.fill)\n",
    "    \n",
    "xtrain = DataFrameImputer().fit_transform( train )\n",
    "xtest = DataFrameImputer().fit_transform( test )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#factorize categorical columns:\n",
    "for column in xtrain:\n",
    "    if xtrain[column].dtype == 'O':\n",
    "#         print pd.factorize(xtrain[column])\n",
    "        xtrain[column] = pd.factorize(xtrain[column])[0]\n",
    "    \n",
    "for column in xtest:\n",
    "    if xtest[column].dtype == 'O':\n",
    "#         print pd.factorize(xtrain[column])\n",
    "        xtest[column] = pd.factorize(xtest[column])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up: scaling/transforms/get_dummies/dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRADIENT BOOSTING & CROSS VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hugobowne-anderson/repos/scikit-learn/sklearn/cross_validation.py:42: DeprecationWarning: This module has been deprecated in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#check this out: http://xgboost.readthedocs.org/en/latest/model.html\n",
    "from sklearn.cross_validation import KFold, train_test_split\n",
    "X = xtrain.values\n",
    "y = target.values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1 , random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Will train until validation_0 error hasn't decreased in 50 rounds.\n",
      "[0]\tvalidation_0-logloss:0.660512\n",
      "[1]\tvalidation_0-logloss:0.633833\n",
      "[2]\tvalidation_0-logloss:0.611452\n",
      "[3]\tvalidation_0-logloss:0.592876\n",
      "[4]\tvalidation_0-logloss:0.577408\n",
      "[5]\tvalidation_0-logloss:0.564382\n",
      "[6]\tvalidation_0-logloss:0.553405\n",
      "[7]\tvalidation_0-logloss:0.543860\n",
      "[8]\tvalidation_0-logloss:0.536023\n",
      "[9]\tvalidation_0-logloss:0.528992\n",
      "[10]\tvalidation_0-logloss:0.523194\n",
      "[11]\tvalidation_0-logloss:0.518142\n",
      "[12]\tvalidation_0-logloss:0.514029\n",
      "[13]\tvalidation_0-logloss:0.510511\n",
      "[14]\tvalidation_0-logloss:0.507166\n",
      "[15]\tvalidation_0-logloss:0.504451\n",
      "[16]\tvalidation_0-logloss:0.502286\n",
      "[17]\tvalidation_0-logloss:0.500022\n",
      "[18]\tvalidation_0-logloss:0.498165\n",
      "[19]\tvalidation_0-logloss:0.496556\n",
      "[20]\tvalidation_0-logloss:0.495312\n",
      "[21]\tvalidation_0-logloss:0.493927\n",
      "[22]\tvalidation_0-logloss:0.492970\n",
      "[23]\tvalidation_0-logloss:0.491965\n",
      "[24]\tvalidation_0-logloss:0.491137\n",
      "[25]\tvalidation_0-logloss:0.490338\n",
      "[26]\tvalidation_0-logloss:0.489665\n",
      "[27]\tvalidation_0-logloss:0.489089\n",
      "[28]\tvalidation_0-logloss:0.488625\n",
      "[29]\tvalidation_0-logloss:0.488127\n",
      "[30]\tvalidation_0-logloss:0.487602\n",
      "[31]\tvalidation_0-logloss:0.487334\n",
      "[32]\tvalidation_0-logloss:0.486997\n",
      "[33]\tvalidation_0-logloss:0.486487\n",
      "[34]\tvalidation_0-logloss:0.486237\n",
      "[35]\tvalidation_0-logloss:0.485890\n",
      "[36]\tvalidation_0-logloss:0.485579\n",
      "[37]\tvalidation_0-logloss:0.485430\n",
      "[38]\tvalidation_0-logloss:0.485202\n",
      "[39]\tvalidation_0-logloss:0.484802\n",
      "[40]\tvalidation_0-logloss:0.484583\n",
      "[41]\tvalidation_0-logloss:0.484348\n",
      "[42]\tvalidation_0-logloss:0.484242\n",
      "[43]\tvalidation_0-logloss:0.483940\n",
      "[44]\tvalidation_0-logloss:0.483843\n",
      "[45]\tvalidation_0-logloss:0.483686\n",
      "[46]\tvalidation_0-logloss:0.483566\n",
      "[47]\tvalidation_0-logloss:0.483306\n",
      "[48]\tvalidation_0-logloss:0.482803\n",
      "[49]\tvalidation_0-logloss:0.482678\n",
      "[50]\tvalidation_0-logloss:0.482645\n",
      "[51]\tvalidation_0-logloss:0.482599\n",
      "[52]\tvalidation_0-logloss:0.482483\n",
      "[53]\tvalidation_0-logloss:0.482222\n",
      "[54]\tvalidation_0-logloss:0.482145\n",
      "[55]\tvalidation_0-logloss:0.482033\n",
      "[56]\tvalidation_0-logloss:0.481891\n",
      "[57]\tvalidation_0-logloss:0.481790\n",
      "[58]\tvalidation_0-logloss:0.481366\n",
      "[59]\tvalidation_0-logloss:0.481314\n",
      "[60]\tvalidation_0-logloss:0.481271\n",
      "[61]\tvalidation_0-logloss:0.481124\n",
      "[62]\tvalidation_0-logloss:0.481000\n",
      "[63]\tvalidation_0-logloss:0.480625\n",
      "[64]\tvalidation_0-logloss:0.480605\n",
      "[65]\tvalidation_0-logloss:0.480526\n",
      "[66]\tvalidation_0-logloss:0.480480\n",
      "[67]\tvalidation_0-logloss:0.480338\n",
      "[68]\tvalidation_0-logloss:0.480286\n",
      "[69]\tvalidation_0-logloss:0.480209\n",
      "[70]\tvalidation_0-logloss:0.480185\n",
      "[71]\tvalidation_0-logloss:0.480126\n",
      "[72]\tvalidation_0-logloss:0.480102\n",
      "[73]\tvalidation_0-logloss:0.479896\n",
      "[74]\tvalidation_0-logloss:0.479858\n",
      "[75]\tvalidation_0-logloss:0.479808\n",
      "[76]\tvalidation_0-logloss:0.479728\n",
      "[77]\tvalidation_0-logloss:0.479648\n",
      "[78]\tvalidation_0-logloss:0.479611\n",
      "[79]\tvalidation_0-logloss:0.479599\n",
      "[80]\tvalidation_0-logloss:0.479577\n",
      "[81]\tvalidation_0-logloss:0.479537\n",
      "[82]\tvalidation_0-logloss:0.479479\n",
      "[83]\tvalidation_0-logloss:0.479466\n",
      "[84]\tvalidation_0-logloss:0.479452\n",
      "[85]\tvalidation_0-logloss:0.479426\n",
      "[86]\tvalidation_0-logloss:0.479434\n",
      "[87]\tvalidation_0-logloss:0.479411\n",
      "[88]\tvalidation_0-logloss:0.479377\n",
      "[89]\tvalidation_0-logloss:0.479222\n",
      "[90]\tvalidation_0-logloss:0.479141\n",
      "[91]\tvalidation_0-logloss:0.479086\n",
      "[92]\tvalidation_0-logloss:0.479091\n",
      "[93]\tvalidation_0-logloss:0.479068\n",
      "[94]\tvalidation_0-logloss:0.479094\n",
      "[95]\tvalidation_0-logloss:0.479089\n",
      "[96]\tvalidation_0-logloss:0.479038\n",
      "[97]\tvalidation_0-logloss:0.479036\n",
      "[98]\tvalidation_0-logloss:0.478962\n",
      "[99]\tvalidation_0-logloss:0.478884\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,\n",
       "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
       "       min_child_weight=1, missing=None, n_estimators=100, nthread=-1,\n",
       "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
       "       scale_pos_weight=1, seed=0, silent=True, subsample=1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Early-stopping\n",
    "#http://xgboost.readthedocs.org/en/latest/python/python_intro.html#early-stopping\n",
    "#Also see https://github.com/dmlc/xgboost/blob/master/demo/guide-python/sklearn_examples.py (Jamie Hall et al.)\n",
    "clf = xgb.XGBClassifier()\n",
    "clf.fit(X_train, y_train, early_stopping_rounds=50, eval_metric=\"logloss\",\n",
    "        eval_set=[(X_test, y_test)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preds = clf.predict_proba(xtest.values, ntree_limit=clf.best_iteration)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "predictions_file = open(\"xgboost_predictions.csv\", \"w\")\n",
    "open_file_object = csv.writer(predictions_file)\n",
    "open_file_object.writerow([\"ID\", \"PredictedProb\"])\n",
    "open_file_object.writerows(zip(IDs, preds))\n",
    "predictions_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This above performed okay: logloss = -0.5252. But I think we need to increase num_rounds and at least try to change preprocessing:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING ANOTHER APPROACH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading & preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data...\n",
      "Clearing...\n"
     ]
    }
   ],
   "source": [
    "#https://www.kaggle.com/director/bnp-paribas-cardif-claims-management/simple-xgboost-0-46146/code\n",
    "print('Load data...')\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "target = train['target']\n",
    "train = train.drop(['ID','target'],axis=1)\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "ids = test['ID'].values\n",
    "test = test.drop(['ID'],axis=1)\n",
    "#\n",
    "print('Clearing...')\n",
    "for (train_name, train_series), (test_name, test_series) in zip(train.iteritems(),test.iteritems()):\n",
    "    if train_series.dtype == 'O':\n",
    "        #for objects: factorize\n",
    "        train[train_name], tmp_indexer = pd.factorize(train[train_name])\n",
    "        test[test_name] = tmp_indexer.get_indexer(test[test_name])\n",
    "        #but now we have -1 values (NaN)\n",
    "    else:\n",
    "        #for int or float: fill NaN\n",
    "        tmp_len = len(train[train_series.isnull()])\n",
    "        if tmp_len>0:\n",
    "            train.loc[train_series.isnull(), train_name] = train_series.mean()\n",
    "        #and Test\n",
    "        tmp_len = len(test[test_series.isnull()])\n",
    "        if tmp_len>0:\n",
    "            test.loc[test_series.isnull(), test_name] = train_series.mean()  #TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A little function to report best scores (from cross validation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "# Utility function to report best scores\n",
    "def report(grid_scores, n_top=3):\n",
    "    top_scores = sorted(grid_scores, key=itemgetter(1), reverse=True)[:n_top]\n",
    "    for i, score in enumerate(top_scores):\n",
    "        print(\"Model with rank: {0}\".format(i + 1))\n",
    "        print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "              score.mean_validation_score,\n",
    "              np.std(score.cv_validation_scores)))\n",
    "        print(\"Parameters: {0}\".format(score.parameters))\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we perform a randomizedsearchCV over a number of parameters (using xgb.XGBClassifier()) --\n",
    "I do this because I don't know how to do it with xgb.train() --\n",
    "Important question: what is the relation between these two xgb.train() & xgb.XGBClassifier()?\n",
    "This is important because I can only do hyperparameter tuning on the latter AND I can only alter num_rounds on the former (which is necessary for a good model, it seems). Any thoughts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5031.24653196\n",
      "Model with rank: 1\n",
      "Mean validation score: -0.515 (std: 0.000)\n",
      "Parameters: {'objective': 'binary:logistic', 'subsample': 0.80000000000000004, 'learning_rate': 0.01, 'colsample_bytree': 0.90000000000000002, 'max_depth': 11}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: -0.516 (std: 0.000)\n",
      "Parameters: {'objective': 'binary:logistic', 'subsample': 0.40000000000000002, 'learning_rate': 0.01, 'colsample_bytree': 1.0, 'max_depth': 12}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation score: -0.516 (std: 0.000)\n",
      "Parameters: {'objective': 'binary:logistic', 'subsample': 0.5, 'learning_rate': 0.01, 'colsample_bytree': 0.90000000000000002, 'max_depth': 13}\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# X = train.values\n",
    "# y = target.values\n",
    "#https://www.kaggle.com/c/springleaf-marketing-response/forums/t/16627/help-with-xgboost-sklearn-randomized-grid-search\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "t0 = time.time()\n",
    "#http://scikit-learn.org/stable/auto_examples/model_selection/randomized_search.html\n",
    "#http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.RandomizedSearchCV.html\n",
    "param_grid = {'max_depth': range(4,15),\n",
    "#                         'min_child_weight': [1,40],\n",
    "                      'objective':['binary:logistic'],\n",
    "#                       'n_estimators':[5],\n",
    "                      'learning_rate':[0.01], #this is same as eta\n",
    "                      'subsample': np.arange(0.1,1.1,0.1),\n",
    "                      'colsample_bytree': np.arange(0.1,1.1,0.1),\n",
    "                      #'scale_pos_weight': [0.5, 1]\n",
    "                      #'model__eta':[0.01,0.02],\n",
    "                     #'model__scale_pos_weight':[0.8,1.0]\n",
    "                      #'model__silent':[1],\n",
    "                      }\n",
    "\n",
    "\n",
    "from sklearn.grid_search import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn import metrics\n",
    "\n",
    "xgb_model = xgb.XGBClassifier()\n",
    "n_iter_search=20\n",
    "random_search = RandomizedSearchCV(xgb_model, param_distributions=param_grid,\n",
    "                                   n_iter=n_iter_search, scoring =\"log_loss\")\n",
    "\n",
    "# start = time()\n",
    "# training and y_training are \n",
    "# small dataset and target variable that I generated from the training dataset\n",
    "random_search.fit(train, target) \n",
    "t1 = time.time()\n",
    "total_time = t1 - t0\n",
    "print total_time\n",
    "\n",
    "print report(random_search.grid_scores_)\n",
    "xgb_model_best = xgb.XGBClassifier()\n",
    "xgb_model_best.set_params(**random_search.best_params_)\n",
    "#http://stackoverflow.com/questions/34674797/xgboost-xgbclassifier-defaults-in-python\n",
    "xgb_model_best.fit(X , y)\n",
    "preds = xgb_model_best.predict_proba(xtest.values)[:,1]\n",
    "#also see this! https://www.kaggle.com/c/airbnb-recruiting-new-user-bookings/forums/t/18494/gridsearchcv-on-xgboost/105272"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "predictions_file = open(\"xgb_rgs_larger_predictions.csv\", \"w\")\n",
    "open_file_object = csv.writer(predictions_file)\n",
    "open_file_object.writerow([\"ID\", \"PredictedProb\"])\n",
    "open_file_object.writerows(zip(IDs, preds))\n",
    "predictions_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This above performed ok (logloss = -.53791) but not as well as other people's xgbtrain() w/ a large num_rounds. For example, see:\n",
    "https://www.kaggle.com/director/bnp-paribas-cardif-claims-management/simple-xgboost-0-46146/code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/mpearmain/homesite-quote-conversion/xgboost-benchmark\n",
    "#https://www.kaggle.com/c/springleaf-marketing-response/forums/t/17089/beating-the-benchmark/96855\n",
    "#https://github.com/lenguyenthedat/kaggle-for-fun/blob/master/springleaf-marketing-response/springleaf-xgb-native.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now I'll try using the best parameters for xgb.XGBClassifier() in xgb.train() AND make num_boost_round = 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cf https://www.kaggle.com/director/bnp-paribas-cardif-claims-management/simple-xgboost-0-46146/code\n",
    "t0 = time.time()\n",
    "xgtrain = xgb.DMatrix(train.values, target.values)\n",
    "xgtest = xgb.DMatrix(test.values)\n",
    "\n",
    "#Now let's fit the model\n",
    "print('Fit the model...')\n",
    "boost_round = 2000 #1800 CHANGE THIS BEFORE START\n",
    "clf = xgb.train(random_search.best_params_,xgtrain,num_boost_round=boost_round,verbose_eval=True,maximize=False)\n",
    "\n",
    "#Make predict\n",
    "print('Predict...')\n",
    "preds = clf.predict(xgtest, ntree_limit=clf.best_iteration )\n",
    "##check here for eval metrics + https://github.com/dmlc/xgboost/blob/master/demo/guide-python/evals_result.py\n",
    "t1 = time.time()\n",
    "total_time = t1 - t0\n",
    "print total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "predictions_file = open(\"xgb_rgs_more_rounds_predictions.csv\", \"w\")\n",
    "open_file_object = csv.writer(predictions_file)\n",
    "open_file_object.writerow([\"ID\", \"PredictedProb\"])\n",
    "open_file_object.writerows(zip(IDs, preds))\n",
    "predictions_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This performed well: logloss = -0.45991 . "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
